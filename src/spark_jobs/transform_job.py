from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_timestamp, avg, stddev, current_timestamp
from pyspark.sql.window import Window
import os
import sys

# Setup ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import config
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
from src.utils.db_connector import load_config

def create_spark_session():
    """Kh·ªüi t·∫°o Spark Session c√≥ k√®m JDBC Driver"""
    return SparkSession.builder \
        .appName("StockDataTransformation") \
        .config("spark.jars", "/opt/jars/postgresql-42.6.0.jar") \
        .config("spark.driver.extraClassPath", "/opt/jars/postgresql-42.6.0.jar") \
        .getOrCreate()

def run_job():
    # 1. Load Config DB
    # L∆∞u √Ω: Spark ch·∫°y trong Docker n√™n host l√† 'postgres' ch·ª© kh√¥ng ph·∫£i localhost
    # Ta d√πng trick os.getenv ƒë·ªÉ l·∫•y c·∫•u h√¨nh t·ª´ bi·∫øn m√¥i tr∆∞·ªùng Docker
    db_host = os.getenv("DB_HOST", "postgres")
    db_user = os.getenv("POSTGRES_USER", "user")
    db_pass = os.getenv("POSTGRES_PASSWORD", "password")
    db_name = os.getenv("POSTGRES_DB", "stockdb")
    
    jdbc_url = f"jdbc:postgresql://{db_host}:5432/{db_name}"
    db_properties = {
        "user": db_user,
        "password": db_pass,
        "driver": "org.postgresql.Driver"
    }

    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN") # Gi·∫£m b·ªõt log r√°c

    print("‚è≥ Reading data from Raw Table...")
    
    # 2. EXTRACT: ƒê·ªçc d·ªØ li·ªáu t·ª´ b·∫£ng raw
    # pushdown_query d√πng ƒë·ªÉ ch·ªâ l·∫•y d·ªØ li·ªáu m·ªõi nh·∫•t (v√≠ d·ª•) ho·∫∑c filter tr∆∞·ªõc
    raw_df = spark.read.jdbc(url=jdbc_url, table="raw_stock_data", properties=db_properties)

    # 3. TRANSFORM
    print("üîÑ Transforming data...")
    
    # a. √âp ki·ªÉu d·ªØ li·ªáu (String -> Timestamp/Double)
    df_clean = raw_df.withColumn("event_time", to_timestamp(col("record_time"))) \
                     .withColumn("close_price", col("close_price").cast("double")) \
                     .withColumn("volume", col("volume").cast("long"))

    # b. T·∫°o Window Spec ƒë·ªÉ t√≠nh to√°n theo t·ª´ng m√£ ch·ª©ng kho√°n, s·∫Øp x·∫øp theo th·ªùi gian
    # Window n√†y d√πng ƒë·ªÉ t√≠nh Moving Average (MA)
    window_spec = Window.partitionBy("symbol").orderBy("event_time").rowsBetween(-4, 0) # 5 d√≤ng g·∫ßn nh·∫•t
    window_spec_20 = Window.partitionBy("symbol").orderBy("event_time").rowsBetween(-19, 0) # 20 d√≤ng g·∫ßn nh·∫•t

    # c. T√≠nh to√°n ch·ªâ s·ªë k·ªπ thu·∫≠t
    final_df = df_clean.withColumn("sma_5", avg("close_price").over(window_spec)) \
                       .withColumn("sma_20", avg("close_price").over(window_spec_20)) \
                       .withColumn("volatility", stddev("close_price").over(window_spec)) \
                       .withColumn("processed_at", current_timestamp())

    # Ch·ªçn c√°c c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ l∆∞u
    final_output = final_df.select(
        "symbol", "event_time", "close_price", "volume", 
        "sma_5", "sma_20", "volatility", "processed_at"
    )

    # 4. LOAD: Ghi v√†o b·∫£ng m·ªõi 'processed_stocks'
    print("üíæ Writing to Processed Table...")
    
    final_output.write.jdbc(
        url=jdbc_url,
        table="processed_stocks",
        mode="append", # append: n·ªëi th√™m, overwrite: ghi ƒë√®
        properties=db_properties
    )
    
    print("‚úÖ Job Finished Successfully!")
    spark.stop()

if __name__ == "__main__":
    run_job()