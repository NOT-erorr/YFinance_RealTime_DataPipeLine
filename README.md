# Financial Data Pipeline: Kafka, Redshift, PySpark & PostgreSQL

## ğŸ“– Giá»›i thiá»‡u
Dá»± Ã¡n nÃ y xÃ¢y dá»±ng má»™t Data Pipeline toÃ n diá»‡n Ä‘á»ƒ thu tháº­p, lÆ°u trá»¯ vÃ  xá»­ lÃ½ dá»¯ liá»‡u chá»©ng khoÃ¡n. Há»‡ thá»‘ng sá»­ dá»¥ng **Apache Kafka** Ä‘á»ƒ streaming dá»¯ liá»‡u tá»« **Yahoo Finance**, lÆ°u trá»¯ dá»¯ liá»‡u thÃ´ vÃ o **AWS Redshift**, sau Ä‘Ã³ sá»­ dá»¥ng **PySpark** Ä‘á»ƒ lÃ m sáº¡ch vÃ  tÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ ká»¹ thuáº­t trÆ°á»›c khi lÆ°u vÃ o **PostgreSQL** Ä‘á»ƒ phá»¥c vá»¥ phÃ¢n tÃ­ch hoáº·c hiá»ƒn thá»‹ lÃªn Dashboard.

## ğŸ— Kiáº¿n trÃºc há»‡ thá»‘ng
Luá»“ng dá»¯ liá»‡u (Data Flow):
1.  **Source:** `yfinance` API (Python).
2.  **Message Queue:** Apache Kafka (Topic: `stock_market_data`).
3.  **Data Warehouse (Raw):** AWS Redshift (LÆ°u trá»¯ dá»¯ liá»‡u thÃ´ nháº­n tá»« Kafka).
4.  **Processing:** PySpark (Äá»c tá»« Redshift -> Xá»­ lÃ½/Transform -> Ghi ra Postgres).
5.  **Serving Database:** PostgreSQL (LÆ°u dá»¯ liá»‡u Ä‘Ã£ qua xá»­ lÃ½).

## ğŸ“‚ Cáº¥u trÃºc dá»± Ã¡n
```text
project-root/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ kafka_config.properties    # Cáº¥u hÃ¬nh Kafka Producer/Consumer
â”‚   â””â”€â”€ database.ini               # Cáº¥u hÃ¬nh káº¿t ná»‘i Redshift/Postgres
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producers/
â”‚   â”‚   â””â”€â”€ stock_producer.py      # Script láº¥y data tá»« yfinance Ä‘áº©y vÃ o Kafka
â”‚   â”œâ”€â”€ consumers/
â”‚   â”‚   â””â”€â”€ redshift_sink.py       # (Hoáº·c config Kafka Connect) Äáº©y data vÃ o Redshift
â”‚   â”œâ”€â”€ spark_jobs/
â”‚   â”‚   â””â”€â”€ transform_job.py       # PySpark ETL logic
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ db_connector.py        # Helper connect DB
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yaml        # Kafka, Zookeeper, Postgres (Local dev)
â”‚   â””â”€â”€ Dockerfile                 # Image cho Spark/Producer
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md